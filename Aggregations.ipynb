{"cells":[{"cell_type":"code","source":["#Aggregating is the act of collecting something together and is a cornerstone of big data analytics.In an aggregation, you will specify a key or grouping and an aggregation function that specifies how you should transform one or more columns. This function must produce one result for each group, given multiple input values. \n\n#The simplest grouping is to just summarize a complete DataFrame by performing an aggregation in a select statement.\n\n#A “group by” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns.\n\n#A “window” gives you the ability to specify one or more keys as well as one or more aggregation functions to transform the value columns. However, the rows input to the function are somehow related to the current row.\n\n#A “grouping set,” which you can use to aggregate at multiple different levels. Grouping sets are available as a primitive in SQL and via rollups and cubes in DataFrames.\n\n# “rollup” makes it possible for you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized hierarchically.\n\n#A “cube” allows you to specify one or more keys as well as one or more aggregation functions to transform the value columns, which will be summarized across all combinations of columns."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"4960a9bb-6107-4611-8655-cca4bbe42be0","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["df = spark.read.format(\"csv\").option(\"header\",\"true\").option(\"inferschema\",\"true\").load(\"dbfs:/FileStore/shared_uploads/annemchandrareddy123@gmail.com/*.csv\").coalesce(5)\ndf.cache()\ndf.createOrReplaceTempView(\"dfTable\")\ndf.show(10)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"cc73711f-9aac-4ea1-bef9-211993189704","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n|InvoiceNo|StockCode|         Description|Quantity|    InvoiceDate|UnitPrice|CustomerID|       Country|\n+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\n|   536365|     null|WHITE HANGING HEA...|       6|01-12-2010 8.26|     2.55|     17850|United Kingdom|\n|   536365|    71053| WHITE METAL LANTERN|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n|   536365|   84406B|CREAM CUPID HEART...|       8|01-12-2010 8.26|     2.75|     17850|United Kingdom|\n|   536365|   84029G|KNITTED UNION FLA...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n|   536365|   84029E|RED WOOLLY HOTTIE...|       6|01-12-2010 8.26|     3.39|     17850|United Kingdom|\n|   536365|    22752|SET 7 BABUSHKA NE...|       2|01-12-2010 8.26|     7.65|     17850|United Kingdom|\n|   536365|    21730|GLASS STAR FROSTE...|       6|01-12-2010 8.26|     4.25|     17850|United Kingdom|\n|   536366|    22633|HAND WARMER UNION...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n|   536366|    22632|HAND WARMER RED P...|       6|01-12-2010 8.28|     1.85|     17850|United Kingdom|\n|   536367|    84879|ASSORTED COLOUR B...|      32|01-12-2010 8.34|     1.69|     13047|United Kingdom|\n+---------+---------+--------------------+--------+---------------+---------+----------+--------------+\nonly showing top 10 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#As mentioned, basic aggregations apply to an entire DataFrame. The simplest example is the count method:\nprint(df.count())\n\n#If you’ve been reading this book chapter by chapter, you know that count is actually an action as opposed to a transformation, and so it returns immediately. You can use count to get an idea of the total size of your dataset but another common pattern is to use it to cache an entire DataFrame in memory, just like we did in this example.\n\n#Now, this method is a bit of an outlier because it exists as a method (in this case) as opposed to a function and is eagerly evaluated instead of a lazy transformation. In the next section, we will see count used as a lazy function, as well.\n\n#The first function worth going over is count, except in this example it will perform as a transformation instead of an action.\nfrom pyspark.sql.functions import count\ndf.select(count(\"StockCode\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"8a29d657-6f33-4a31-bbcc-44867b65407e","inputWidgets":{},"title":"Count"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["1108363\n+----------------+\n|count(StockCode)|\n+----------------+\n|         1108362|\n+----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#There are a number of gotchas when it comes to null values and counting. For instance, when performing a count(*), Spark will count null values (including rows containing all nulls). However, when counting an individual column, Spark will not count the null values."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"fd029c16-1e4c-4c50-84c1-67bee6019394","inputWidgets":{},"title":"#WARNING"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import countDistinct\ndf.select(countDistinct(\"StockCode\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"a8f424d8-7dbb-44ad-b1cb-4cb5fd360d41","inputWidgets":{},"title":"countDistinct"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4385|\n+-------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#approx_count_distinct\n#Often, we find ourselves working with large datasets and the exact distinct count is irrelevant. There are times when an approximation to a certain degree of accuracy will work just fine, and for that, you can use the approx_count_distinct function:\nfrom pyspark.sql.functions import approx_count_distinct\ndf.select(approx_count_distinct(\"StockCode\", 0.1)).show()#0.01\n\n#You will notice that approx_count_distinct took another parameter with which you can specify the maximum estimation error allowed. In this case, we specified a rather large error and thus receive an answer that is quite far off but does complete more quickly than countDistinct. You will see much greater performance gains with larger datasets."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"097a0860-570d-4c34-a69b-2f2f28608117","inputWidgets":{},"title":"approx_count_distinct"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------------------+\n|approx_count_distinct(StockCode)|\n+--------------------------------+\n|                            3633|\n+--------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import first, last, min, max, sum, sumDistinct, avg, mean, expr\ndf.select(first(\"StockCode\"),last(\"StockCode\")).show()\n\ndf.select(min(\"Quantity\"), max(\"Quantity\")).show()\n\ndf.select(sum(\"Quantity\")).show()\n\ndf.select(sumDistinct(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"67f379e0-8175-47a3-a881-8239bbdb8792","inputWidgets":{},"title":"first and last || min and max||sum and sumDistinct"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------+---------------+\n|first(StockCode)|last(StockCode)|\n+----------------+---------------+\n|            null|  United States|\n+----------------+---------------+\n\n+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|    and Saba\"|     Zimbabwe|\n+-------------+-------------+\n\n+-------------+\n|sum(Quantity)|\n+-------------+\n|  1.0519548E7|\n+-------------+\n\n"]},{"output_type":"stream","output_type":"stream","name":"stderr","text":["/databricks/spark/python/pyspark/sql/functions.py:723: FutureWarning: Deprecated in 3.2, use sum_distinct instead.\n  warnings.warn(\"Deprecated in 3.2, use sum_distinct instead.\", FutureWarning)\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------------------+\n|sum(DISTINCT Quantity)|\n+----------------------+\n|               29310.0|\n+----------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["df.select(\n    count(\"Quantity\").alias(\"Total_transactions\"),\n    sum(\"Quantity\").alias(\"Total_purchases\"),\n    avg(\"Quantity\").alias(\"avg_of_purchases\"),\n    expr(\"mean(Quantity)\").alias(\"mean_of_purchases\")\n).selectExpr(\"Total_transactions\",\"Total_purchases\",\"avg_of_purchases\",\"mean_of_purchases\").show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"ec83121c-fa13-45d4-b21c-55dbb47348c6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+------------------+---------------+----------------+-----------------+\n|Total_transactions|Total_purchases|avg_of_purchases|mean_of_purchases|\n+------------------+---------------+----------------+-----------------+\n|           1107604|    1.0519548E7|9.50841377116097| 9.50841377116097|\n+------------------+---------------+----------------+-----------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import var_pop, stddev_pop\nfrom pyspark.sql.functions import var_samp, stddev_samp\ndf.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"b8fcb3f7-9198-401e-ba08-1bdc5391eda5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-----------------+------------------+--------------------+---------------------+\n|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+-----------------+------------------+--------------------+---------------------+\n|46702.64952029336| 46702.69173394334|  216.10795802166416|   216.10805568960944|\n+-----------------+------------------+--------------------+---------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\nfrom pyspark.sql.functions import skewness, kurtosis\ndf.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"1d0754af-cad5-4d49-8b78-1819dd96b412","inputWidgets":{},"title":"Skewness and kurtosis"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+------------------+\n|  skewness(Quantity)|kurtosis(Quantity)|\n+--------------------+------------------+\n|-0.33458977206280915|121676.86256142345|\n+--------------------+------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import covar_pop,covar_samp,corr\n\ndf.select(corr(\"InvoiceNo\",\"Quantity\"),covar_samp(\"InvoiceNo\",\"Quantity\"),covar_pop(\"InvoiceNo\",\"Quantity\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"0ea34c30-9a22-4cf0-9468-0c363997f193","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+-------------------------+-------------------------------+------------------------------+\n|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n+-------------------------+-------------------------------+------------------------------+\n|     9.317651161942416E-4|             2013.5817936353692|            2013.5799419467146|\n+-------------------------+-------------------------------+------------------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#In Spark, you can perform aggregations not just of numerical values using formulas, you can also perform them on complex types. For example, we can collect a list of values present in a given column or only the unique values by collecting to a set.\n\n\nfrom pyspark.sql.functions import collect_set, collect_list\ndf.agg(collect_set(\"Country\"), collect_list(\"Country\")).show()\n\n#SELECT collect_set(Country), collect_set(Country) FROM dfTable"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"22ad6bd3-df7a-43c4-b01c-5adb61dec306","inputWidgets":{},"title":"Aggregating to Complex Types"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+--------------------+---------------------+\n|collect_set(Country)|collect_list(Country)|\n+--------------------+---------------------+\n|[Portugal, Italy,...| [United Kingdom, ...|\n+--------------------+---------------------+\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#As we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually we prefer to use the count function. Rather than passing that function as an expression into a select statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions that just need to have some aggregation specified. You can even do things like alias a column after transforming it for later use in your data flow:\n\nfrom pyspark.sql.functions import count\n\ndf.groupBy(\"InvoiceNo\").agg(\n    count(\"Quantity\").alias(\"quan\"),\n    expr(\"count(Quantity)\")).show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"5bb60230-a322-4133-bfb2-03c6866f79b4","inputWidgets":{},"title":"Grouping with Expressions"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+----+---------------+\n|InvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n|   536596|  18|             18|\n|   536938|  42|             42|\n|   537252|   3|              3|\n|   537691|  60|             60|\n|   538041|   3|              3|\n|   538184|  52|             52|\n|   538517| 106|            106|\n|   538879|  38|             38|\n|   539275|  12|             12|\n|   539630|  24|             24|\n|   540499|  48|             48|\n|   540540|  44|             44|\n|  C540850|   2|              2|\n|   540976|  96|             96|\n|   541432|   8|              8|\n|   541518| 202|            202|\n|   541783|  70|             70|\n|   542026|  18|             18|\n|   542375|  12|             12|\n|  C542604|  16|             16|\n+---------+----+---------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Sometimes, it can be easier to specify your transformations as a series of Maps for which the key is the column, and the value is the aggregation function (as a string) that you would like to perform. You can reuse multiple column names if you specify them inline, as well:\n\ndf.groupBy(\"InvoiceNo\").agg(expr(\"avg(Quantity)\"),expr(\"stddev_pop(Quantity)\"))\\\n  .show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"606ce6fc-65d2-486b-af2a-798f82d73bbc","inputWidgets":{},"title":"Grouping with Maps"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+---------+------------------+--------------------+\n|InvoiceNo|     avg(Quantity)|stddev_pop(Quantity)|\n+---------+------------------+--------------------+\n|   536596|               1.5|   1.118033988749895|\n|   536938|33.142857142857146|  20.698023172885524|\n|   537252|              31.0|                 0.0|\n|   537691|              8.15|   5.597097462078001|\n|   538041|              30.0|                 0.0|\n|   538184|12.076923076923077|   8.142590198943392|\n|   538517|3.0377358490566038|  2.3946659604837897|\n|   538879|21.157894736842106|  11.811070444356483|\n|   539275|              26.0|  12.806248474865697|\n|   539630|20.333333333333332|  10.225241100118645|\n|   540499|              3.75|  2.6653642652865788|\n|   540540|2.1363636363636362|  1.0572457590557278|\n|  C540850|              -1.0|                 0.0|\n|   540976|10.520833333333334|   6.496760677872902|\n|   541432|             12.25|  10.825317547305483|\n|   541518| 23.10891089108911|  20.550782784878713|\n|   541783|11.314285714285715|   8.467657556242811|\n|   542026| 7.666666666666667|   4.853406592853679|\n|   542375|               8.0|  3.4641016151377544|\n|  C542604|              -8.0|  15.173990905493518|\n+---------+------------------+--------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["\n#You can also use window functions to carry out some unique aggregations by either computing some aggregation on a specific “window” of data, which you define by using a reference to the current data. This window specification determines which rows will be passed in to this function. Now this is a bit abstract and probably similar to a standard group-by, so let’s differentiate them a bit more.\n\n#A group-by takes data, and every row can go only into one grouping. A window function calculates a return value for every input row of a table based on a group of rows, called a frame. Each row can fall into one or more frames. A common use case is to take a look at a rolling average of some value for which each row represents one day. If you were to do this, each row would end up in seven different frames. We cover defining frames a little later, but for your reference, Spark supports three kinds of window functions: ranking functions, analytic functions, and aggregate functions.\n\nfrom pyspark.sql.functions import col, to_date\ndfWithDate = df.withColumn(\"date\", to_date(col(\"InvoiceDate\"), \"MM/d/yyyy H:mm\"))\ndfWithDate.createOrReplaceTempView(\"dfWithDate\")\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.functions import desc\nwindowSpec = Window\\\n  .partitionBy(\"CustomerId\", \"date\")\\\n  .orderBy(desc(\"Quantity\"))\\\n  .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n\n#e want to use an aggregation function to learn more about each specific customer. An example might be establishing the maximum purchase quantity over all time. To answer this, we use the same aggregation functions that we saw earlier by passing a column name or expression. In addition, we indicate the window specification that defines to which frames of data this function will apply:\nfrom pyspark.sql.functions import max\nmaxPurchaseQuantity = max(col(\"Quantity\")).over(windowSpec)\n\n#You will notice that this returns a column (or expressions). We can now use this in a DataFrame select statement. Before doing so, though, we will create the purchase quantity rank. To do that we use the dense_rank function to determine which date had the maximum purchase quantity for every customer. We use dense_rank as opposed to rank to avoid gaps in the ranking sequence when there are tied values (or in our case, duplicate rows):\nfrom pyspark.sql.functions import dense_rank, rank\npurchaseDenseRank = dense_rank().over(windowSpec)\npurchaseRank = rank().over(windowSpec)\n\nfrom pyspark.sql.functions import col\n\ndfWithDate.where(\"CustomerId IS NOT NULL\").orderBy(\"CustomerId\")\\\n  .select(\n    col(\"CustomerId\"),\n    col(\"date\"),\n    col(\"Quantity\"),\n    purchaseRank.alias(\"quantityRank\"),\n    purchaseDenseRank.alias(\"quantityDenseRank\"),\n    maxPurchaseQuantity.alias(\"maxPurchaseQuantity\")).show()\n"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"313098ef-17ef-424a-a288-618e216840ce","inputWidgets":{},"title":"Window Functions"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+----+--------+------------+-----------------+-------------------+\n|CustomerId|date|Quantity|quantityRank|quantityDenseRank|maxPurchaseQuantity|\n+----------+----+--------+------------+-----------------+-------------------+\n|     12346|null|   74215|           1|                1|              74215|\n|     12346|null|  -74215|           2|                2|              74215|\n|   12346.0|null|   74215|           1|                1|              74215|\n|   12346.0|null|  -74215|           2|                2|              74215|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       8|           1|                1|                  8|\n|     12347|null|       6|          16|                2|                  8|\n+----------+----+--------+------------+-----------------+-------------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":["#Thus far in this chapter, we’ve seen simple group-by expressions that we can use to aggregate on a set of columns with the values in those columns. However, sometimes we want something a bit more complete—an aggregation across multiple groups. We achieve this by using grouping sets. Grouping sets are a low-level tool for combining sets of aggregations together. They give you the ability to create arbitrary aggregation in their group-by statements.\n\ndfNoNull = dfWithDate.drop()\ndfNoNull.createOrReplaceTempView(\"dfNoNull\")\n\nspark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode ORDER BY CustomerId DESC, stockCode DESC\").show()\n\nspark.sql(\"SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode)) ORDER BY CustomerId DESC, stockCode DESC\").show()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":true,"cellMetadata":{"rowLimit":10000,"byteLimit":2048000},"nuid":"75c1c67d-0a6c-4c4a-bce7-40b48e24cce5","inputWidgets":{},"title":"Grouping Sets"}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["+----------+-----------+-------------+\n|CustomerId|  stockCode|sum(Quantity)|\n+----------+-----------+-------------+\n|       Yes|99999999999|         null|\n|       Yes|          8|         null|\n|       Yes|         72|         null|\n|       Yes|         57|         null|\n|       Yes|         56|         null|\n|       Yes|         55|         null|\n|       Yes|         54|         null|\n|       Yes|         50|         null|\n|       Yes|         49|         null|\n|       Yes|         48|         null|\n|       Yes|         47|         null|\n|       Yes|         46|         null|\n|       Yes|         45|         null|\n|       Yes|         44|         null|\n|       Yes|         43|         null|\n|       Yes|         42|         null|\n|       Yes|         41|         null|\n|       Yes|         40|         null|\n|       Yes|         39|         null|\n|       Yes|         38|         null|\n+----------+-----------+-------------+\nonly showing top 20 rows\n\n+----------+-----------+-------------+\n|customerId|  stockCode|sum(Quantity)|\n+----------+-----------+-------------+\n|       Yes|99999999999|         null|\n|       Yes|          8|         null|\n|       Yes|         72|         null|\n|       Yes|         57|         null|\n|       Yes|         56|         null|\n|       Yes|         55|         null|\n|       Yes|         54|         null|\n|       Yes|         50|         null|\n|       Yes|         49|         null|\n|       Yes|         48|         null|\n|       Yes|         47|         null|\n|       Yes|         46|         null|\n|       Yes|         45|         null|\n|       Yes|         44|         null|\n|       Yes|         43|         null|\n|       Yes|         42|         null|\n|       Yes|         41|         null|\n|       Yes|         40|         null|\n|       Yes|         39|         null|\n|       Yes|         38|         null|\n+----------+-----------+-------------+\nonly showing top 20 rows\n\n"]}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"740e0149-ae78-40b0-b52a-c9aec8685d6e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"bbda6885-bc40-4ed9-a266-53ce4e026534","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Aggregations","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{}}},"nbformat":4,"nbformat_minor":0}
